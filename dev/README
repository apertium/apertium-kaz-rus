
Installation
---------------------------------------------------------------------
To install apertium-transfer-tools-generalisation you need to have installed the
following packages:

libxml2, at least version 2.6.17
libpcre, at least version 6.4

lttoolbox, at least version 3.2.0*
apertium, at least version 3.2.0*

* You'll need to patch (with the command "patch -p0 < PATCH_FILE") the source code of these packages with patches
provided in the directory "./apertium-patch", recompile, and reinstall them.

moreutils (http://joeyh.name/code/moreutils/), which provides the command-line tool "parallel"

python, at least version 2.6.7,
with packages:
  - PuLP (http://pythonhosted.org/PuLP/main/installing_pulp_at_home.html)
  - argparse

Once you have all the required software installed, install
the C programs of apertium-transfer-tools-generalisation in the common way:

$ cd phrase-extraction
$ ./autogen.sh 
$ ./configure && make

and as a root

# make install

The python programs of apertium-transfer-tools-generalisation do not require any installation.


Configuration
-----------------------------------------------------------------------

**** Please update all the configuration files described in this section before running the
rule extraction algorithm. **********

Bilingual phrase extraction:

- When analysing the parallel corpus with Apertium, some additional operations may be defined in
these files found in ./phrase-extraction/transfer-tools-scripts:

  * preprocess_l1_l1-l2.sh : Additonal operations when analysing the l1 side for the l1-l2 language pair
  * preprocess_l2_l1-l2.sh : Additonal operations when analysing the l2 side for the l1-l2 language pair
  * preprocess_l1_l2-l1.sh : Additonal operations when analysing the l1 side for the l2-l1 language pair
  * preprocess_l2_l2-l1.sh : Additonal operations when analysing the l2 side for the l2-l1 language pair

These files MUST exist for the phrase extraction to correctly be carried out. If you do not
want to do additional operations in the analysis, please simply put a "cat" command inside the files (see preprocess_en_en-es.sh)
See the files for the es-ca and ca-es language pairs to obtain some examples of operations whihc need to be carried out after the 
analysis.

- Extraction of bilingual phrases from the parallel corpus is guided by words
of closed lexical categories. These categories are defined in the file:

  * ./phrase-extraction/transfer-tools-scripts/markers
  
When dealing with a new language pair, they may need to be updated.

Generalisation:

- Although Apertium does not explicitly encode the type of each morphological inflection tag (e.g. <f> is a gender and
<sg> is a number), the generalisation algorithm needs this information. Therefore, it must be explicitly defined
in the files:

  * ./rule-generalisation/taggroups_l1-l2
  * ./rule-generalisation/tagsequences_l1-l2
  
The first one defines the morphological inflection tag types and their possible values, for instance, "gender:m,f,mf,GD,nt".
When the same tag belongs to different tag types, the lexical category is used to break the ambiguity. For instance, "<pos>"
could mean that a determiner is possessive or that an adjective is possessive:
determinertype:def,ind,dem,pos:det
adjtype:ind,itg,pos,sup:adj

Once all the types of tags are defined, the sequences of types of tags which the lexical forms of each lexical category
should contain are defined in the second file. For instance, in the Spanish-Catalan language pair, adjectives contain
an adjective type, a gender and a number: "adj:adjtype,gender,numberat". 

In some cases some tags may not be present. For instance, possessive adjectives contain the tag "<pos>",
but common adjectives do not contain a tag specifying the type of adjective. It is not a problem, since, in that case, 
the algorithm will create a special "empty" tag. 

Note also that the set of tag types and sequences must be the same for both languages of the pair (see the configuration
of the English-Spanish language pair for examples).

Applying the rules:

- When applying the learned rules, it may be necessary to join some lexical forms generated by them to allow the
Apertium generation module to correctly inflect them. For instance, verbs and enclitic pronouns are 
found as a single unit in the Apertium monolingual dictionaries. The lexical forms to be joined are
defined in the file:

  * ./phrase-extraction/transfer-tools-scripts/apertium-l1-l2.posttransfer.ptx
  
If no lexical forms need to be joined, use a file with no rules, such as "apertium-es-en.posttransfer.ptx".


Running the rule extraction algorithm
-----------------------------------------------------------------------

The rule extraction algorithm needs a user-defined threshold to properly work. This threshold
is the minimum proportion of the bilingual phrases matched by an alignment templates which
must be correctly reproduced by it. The alignment templates whose proportion of matching bilingual
phrases which are correctly reproduced is below the threshold value are discarded.
If a development corpus is provided, the program will choose the most appropriate
threshold from a range.

USAGE: extract-rules.sh flags
flags:
  -s,--source_language:  source language language (default: 'es')
  -t,--target_language:  target language (default: 'ca')
  -c,--corpus:  prefix of files containing parallel corpus (suffixes are .SL and .TL) (COMPULSORY)
  -g,--giza_dir:  Giza++ directory (default: '~/giza-pp/GIZA++-v2')
  -d,--data_dir:  Directory where the sources of the Apertium dictionaries can be found (default: /usr/local/share/apertium/apertium-SL-TL/)
  -u,--apertium_prefix:  Prefix where Apertium was installed (usually, /usr/local/)
  -m,--tmp_dir:  Directory where the results will be stored (If this parameter is not set, a new directory under /tmp/ will be created)
  -y,--[no]segment_by_marker: Segment SL corpus according to marker hypothesis (default: false)
  -f,--filtering_thresholds:  Thresholds for filtering alignment templates. Format is start step end, as in the seq command. A single threshold can also be defined
                              (default: '0.45 0.05 1')
  --test_corpus:  evaluation corpus prefix (suffixes are .SL and .TL) (If it is not set, the learned rules will not be evaluated)
  --dev_corpus:  development corpus prefix (suffixes are .SL and .TL). It is used to choose the most appropriate threshold
                 (If it is not set, a fith part of the trainign corpus can be used by enabling the option --discard_a_fifth_of_corpus )

Result, in alignment template (AT) format, can be found in $DIR/tuning-proportion_correct_bilphrases_thresholdabove2-$THRESHOLD-subrules/result-f-$THRESHOLD.gz
In a nutshell, the extended alignment template formalism used by this approach works as follows. Let the AT in the following line
be one of the ATs generated by the rule extraction algorithm.

<det><def><m><*numberat> <n><empty_tag_ntype><mf><*numberat> | <det><def><m><)000numberat> <n><empty_tag_ntype><mf><)001numberat> | 0:0 1:1 | <det><def><m> <n><empty_tag_ntype><mf>

Fields are separated by '|'. 
- The first field is the sequence of lexical forms matching the AT. It matches
a masculine definite determiner with any number and masculine-feminine common noun with any number.
- The fourth field presents the required values of the inflection tags of the matching words after looking them
up in the bilingual dictionary (defined as restrictions by Sánchez-Martínez and Forcada). Note that the noun
must be masculine-feminine in the target language too. That's why the gender of the noun is not transferred
to the determiner. 
- The second field shows the lexical forms resulting from applying the AT. The tag ")000numberat"
tells us that the number of the lexical form is obtained by looking up in the bilingual dictionary
the SL lexical form in position 0. 
- The third field shows the alignments used to obtain the lemmas of the resulting lexical forms.

The rules in Apertium format can be found at $DIR/tuning-proportion_correct_bilphrases_thresholdabove2-$THRESHOLD-subrules/queries/test-*/experiment/rules/rules.xml
The directory $DIR/tuning-proportion_correct_bilphrases_thresholdabove2-$THRESHOLD-subrules/queries/test-*/experiment/evaluation contains
different evaluation metrics of the extracted rules and some debug information. Some of them are:
* evaluation_learnedrules: BLEU score
* evaluation_norules: BLEU score of word-for-word translation, useful to assess the impact of the learned rules
* table.html : Table with the most applied rules and some examples of the matching segment and its translation


